{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c06dbe",
   "metadata": {},
   "source": [
    "# CIDM Lab Work 1 — P1→P3 (Code-Only Notebook)\n",
    "**Module:** P170M109 Computational Intelligence and Decision Making  \n",
    "**Scope:** Data analysis & preprocessing (P1), KNN/DecisionTree/RandomForest modeling (P2), hyperparameter selection & results (P3).  \n",
    "**Date:** 2025-09-22\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "30fba288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T16:45:43.808103Z",
     "start_time": "2025-09-22T16:45:42.116530Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "733c053e",
   "metadata": {},
   "source": [
    "## P1. Data analysis and preprocessing\n",
    "### 1) Load data"
   ]
  },
  {
   "cell_type": "code",
   "id": "4226057e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T16:45:46.021962Z",
     "start_time": "2025-09-22T16:45:45.661086Z"
    }
   },
   "source": [
    "# Try common paths (100K preferred); adjust if needed.\n",
    "CSV_PATHS = [\n",
    "    \"/mnt/data/apartments_for_rent_classified_100K.csv\",\n",
    "    \"/mnt/data/apartments_for_rent_classified_10K.csv\",\n",
    "    \"apartments_for_rent_classified_100K.csv\",\n",
    "    \"apartments_for_rent_classified_10K.csv\",\n",
    "]\n",
    "\n",
    "def load_first_available(paths):\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            print(\"Loaded:\", p)\n",
    "            return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise FileNotFoundError(\"Place the CSV next to this notebook or in /mnt/data/.\")\n",
    "\n",
    "df_raw = load_first_available(CSV_PATHS)\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "df_raw.head(3)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Place the CSV next to this notebook or in /mnt/data/.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m             \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m     17\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mPlace the CSV next to this notebook or in /mnt/data/.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m df_raw = \u001B[43mload_first_available\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCSV_PATHS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mRaw shape:\u001B[39m\u001B[33m\"\u001B[39m, df_raw.shape)\n\u001B[32m     21\u001B[39m df_raw.head(\u001B[32m3\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mload_first_available\u001B[39m\u001B[34m(paths)\u001B[39m\n\u001B[32m     15\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m     16\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mPlace the CSV next to this notebook or in /mnt/data/.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: Place the CSV next to this notebook or in /mnt/data/."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "6f9f1f18",
   "metadata": {},
   "source": [
    "### 2) Determine data types & build data quality report (separate numeric vs categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d240e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_feature_types(df: pd.DataFrame):\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cat_cols = [c for c in df.columns if c not in num_cols]\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def dqr_numeric(df: pd.DataFrame, cols):\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        n = len(s)\n",
    "        miss = s.isna().sum()\n",
    "        desc = s.describe()\n",
    "        rows.append({\n",
    "            \"feature\": c,\n",
    "            \"n\": n,\n",
    "            \"missing\": int(miss),\n",
    "            \"missing_%\": round(100*miss/n, 2),\n",
    "            \"unique\": int(s.nunique(dropna=True)),\n",
    "            \"mean\": desc.get(\"mean\", np.nan),\n",
    "            \"std\": desc.get(\"std\", np.nan),\n",
    "            \"min\": desc.get(\"min\", np.nan),\n",
    "            \"p25\": desc.get(\"25%\", np.nan),\n",
    "            \"p50\": desc.get(\"50%\", np.nan),\n",
    "            \"p75\": desc.get(\"75%\", np.nan),\n",
    "            \"max\": desc.get(\"max\", np.nan),\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values([\"missing_%\",\"feature\"], ascending=[False, True])\n",
    "\n",
    "def dqr_categorical(df: pd.DataFrame, cols):\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        s = df[c].astype(\"string\")\n",
    "        n = len(s)\n",
    "        miss = s.isna().sum()\n",
    "        rows.append({\n",
    "            \"feature\": c,\n",
    "            \"n\": n,\n",
    "            \"missing\": int(miss),\n",
    "            \"missing_%\": round(100*miss/n, 2),\n",
    "            \"unique\": int(s.nunique(dropna=True)),\n",
    "            \"top_value\": s.value_counts(dropna=True).index[:1].tolist(),\n",
    "            \"top_freq\": s.value_counts(dropna=True).values[:1].tolist(),\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values([\"missing_%\",\"feature\"], ascending=[False, True])\n",
    "\n",
    "num_cols_raw, cat_cols_raw = infer_feature_types(df_raw)\n",
    "print(\"Numeric cols (raw):\", len(num_cols_raw))\n",
    "print(\"Categorical cols (raw):\", len(cat_cols_raw))\n",
    "\n",
    "dqr_num_raw = dqr_numeric(df_raw, num_cols_raw)\n",
    "dqr_cat_raw = dqr_categorical(df_raw, cat_cols_raw)\n",
    "\n",
    "print(\"\\nNumeric DQR (raw):\")\n",
    "display(dqr_num_raw.head(20))\n",
    "print(\"\\nCategorical DQR (raw):\")\n",
    "display(dqr_cat_raw.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ba25c",
   "metadata": {},
   "source": [
    "### 3) Pre-normalize `price` to monthly for **before** plots (do not clean yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_monthly(price, ptype):\n",
    "    t = (str(ptype) if ptype is not None else \"\").strip().lower()\n",
    "    try:\n",
    "        p = float(price)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    if not np.isfinite(p) or p <= 0:\n",
    "        return np.nan\n",
    "    if \"week\" in t:             factor = 52/12\n",
    "    elif \"fortnight\" in t or \"biweek\" in t: factor = 26/12\n",
    "    elif \"day\" in t:            factor = 30\n",
    "    elif \"year\" in t or \"annual\" in t:      factor = 1/12\n",
    "    elif \"hour\" in t:           factor = 24*30\n",
    "    else:                       factor = 1.0  # assume monthly\n",
    "    return p * factor\n",
    "\n",
    "df_before = df_raw.copy()\n",
    "if \"price\" in df_before.columns and \"price_type\" in df_before.columns:\n",
    "    df_before[\"price_monthly\"] = df_before.apply(lambda r: to_monthly(r.get(\"price\"), r.get(\"price_type\")), axis=1)\n",
    "else:\n",
    "    raise KeyError(\"Expected 'price' and 'price_type' columns.\")\n",
    "\n",
    "print(\"Before-shape:\", df_before.shape)\n",
    "df_before[[\"price\",\"price_type\",\"price_monthly\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0edb83",
   "metadata": {},
   "source": [
    "### 4) Distributions (BEFORE preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbde92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for key numeric features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "df_before[\"price_monthly\"].dropna().plot(kind=\"hist\", bins=50, ax=axes[0])\n",
    "axes[0].set_title(\"price_monthly (before)\")\n",
    "if \"square_feet\" in df_before.columns:\n",
    "    pd.to_numeric(df_before[\"square_feet\"], errors=\"coerce\").dropna().plot(kind=\"hist\", bins=50, ax=axes[1])\n",
    "    axes[1].set_title(\"square_feet (before)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "df_before[[\"price_monthly\"]].boxplot(ax=axes[0])\n",
    "axes[0].set_title(\"price_monthly (before)\")\n",
    "if \"square_feet\" in df_before.columns:\n",
    "    pd.to_numeric(df_before[\"square_feet\"], errors=\"coerce\").to_frame().boxplot(ax=axes[1])\n",
    "    axes[1].set_title(\"square_feet (before)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for a categorical (top 15 states/cities if present)\n",
    "for cat in [\"state\",\"cityname\",\"region\"]:\n",
    "    if cat in df_before.columns:\n",
    "        vc = df_before[cat].astype(str).value_counts().head(15)\n",
    "        ax = vc.plot(kind=\"bar\", figsize=(8,3))\n",
    "        ax.set_title(f\"{cat} (top 15)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa5a8e",
   "metadata": {},
   "source": [
    "### 5) Consider derived features & prepare ABT (Analytics Base Table)\n",
    "- `amenity_count`: number of listed amenities (if available)\n",
    "- `pet_cat_allowed` / `pet_dog_allowed` from `pets_allowed`\n",
    "- `price_per_sqft`: `price_monthly / square_feet` (when square_feet available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c13df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_before.copy()\n",
    "\n",
    "# Binary encodings\n",
    "for col in [\"fee\",\"has_photo\"]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = (df_clean[col].astype(str).str.lower()\n",
    "                         .map({\"yes\":1,\"no\":0,\"thumbnail\":1}).fillna(0).astype(int))\n",
    "\n",
    "# Pets flags\n",
    "if \"pets_allowed\" in df_clean.columns:\n",
    "    lower = df_clean[\"pets_allowed\"].fillna(\"\").astype(str).str.lower()\n",
    "    df_clean[\"pet_cat_allowed\"] = lower.str.contains(\"cat\").astype(int)\n",
    "    df_clean[\"pet_dog_allowed\"] = lower.str.contains(\"dog\").astype(int)\n",
    "\n",
    "# Amenity count\n",
    "if \"amenities\" in df_clean.columns:\n",
    "    df_clean[\"amenity_count\"] = df_clean[\"amenities\"].fillna(\"\").apply(lambda x: len(str(x).split(\",\"))).astype(int)\n",
    "\n",
    "# price_per_sqft (guard divide-by-zero)\n",
    "if \"square_feet\" in df_clean.columns:\n",
    "    sf = pd.to_numeric(df_clean[\"square_feet\"], errors=\"coerce\")\n",
    "    df_clean[\"price_per_sqft\"] = df_clean[\"price_monthly\"] / sf.replace(0, np.nan)\n",
    "\n",
    "# ABT candidates: keep numeric + lat/long + simple binaries\n",
    "abt_drop = [\"id\",\"title\",\"body\",\"address\",\"amenities\",\"price_display\",\"price\",\"price_type\",\"currency\",\"source\"]\n",
    "df_abt = df_clean.drop(columns=[c for c in abt_drop if c in df_clean.columns], errors=\"ignore\")\n",
    "\n",
    "print(\"ABT candidate columns:\", df_abt.columns.tolist()[:25], \"... (total:\", len(df_abt.columns), \")\")\n",
    "df_abt.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91273e53",
   "metadata": {},
   "source": [
    "### 6) Perform preprocessing (missing values, filters, outliers)\n",
    "Actions (simple and transparent):\n",
    "- Keep USD (if `currency` exists).\n",
    "- Drop invalid coords (lat/lon outside valid ranges or near 0,0).\n",
    "- Clip extreme outliers: remove top 0.5% by `price_monthly`; keep `square_feet` in [120, 8000].\n",
    "- Fill remaining numeric NaNs with median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe566589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = df_abt.copy()\n",
    "\n",
    "# currency\n",
    "if \"currency\" in df_before.columns:\n",
    "    df_proc = df_proc[df_before[\"currency\"].astype(str).str.upper().eq(\"USD\") | df_before[\"currency\"].isna()]\n",
    "\n",
    "# coords validity (if present)\n",
    "if \"latitude\" in df_proc.columns and \"longitude\" in df_proc.columns:\n",
    "    lat = pd.to_numeric(df_proc[\"latitude\"], errors=\"coerce\")\n",
    "    lon = pd.to_numeric(df_proc[\"longitude\"], errors=\"coerce\")\n",
    "    good = lat.between(-90, 90) & lon.between(-180, 180) & ~((lat.abs()<0.1) & (lon.abs()<0.1))\n",
    "    df_proc = df_proc[good]\n",
    "\n",
    "# square feet sanity (if present)\n",
    "if \"square_feet\" in df_proc.columns:\n",
    "    sf = pd.to_numeric(df_proc[\"square_feet\"], errors=\"coerce\")\n",
    "    df_proc = df_proc[(sf.isna()) | ((sf >= 120) & (sf <= 8000))]\n",
    "\n",
    "# remove top 0.5% by price_monthly\n",
    "q995 = df_proc[\"price_monthly\"].quantile(0.995)\n",
    "df_proc = df_proc[df_proc[\"price_monthly\"] <= q995]\n",
    "\n",
    "# fill numeric NAs with median\n",
    "for c in df_proc.select_dtypes(include=[np.number]).columns:\n",
    "    df_proc[c] = df_proc[c].fillna(df_proc[c].median())\n",
    "\n",
    "print(\"Processed shape:\", df_proc.shape)\n",
    "df_proc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349535b",
   "metadata": {},
   "source": [
    "### 7) Distributions (AFTER preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "df_proc[\"price_monthly\"].dropna().plot(kind=\"hist\", bins=50, ax=axes[0])\n",
    "axes[0].set_title(\"price_monthly (after)\")\n",
    "if \"square_feet\" in df_proc.columns:\n",
    "    pd.to_numeric(df_proc[\"square_feet\"], errors=\"coerce\").dropna().plot(kind=\"hist\", bins=50, ax=axes[1])\n",
    "    axes[1].set_title(\"square_feet (after)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "df_proc[[\"price_monthly\"]].boxplot(ax=axes[0])\n",
    "axes[0].set_title(\"price_monthly (after)\")\n",
    "if \"square_feet\" in df_proc.columns:\n",
    "    pd.to_numeric(df_proc[\"square_feet\"], errors=\"coerce\").to_frame().boxplot(ax=axes[1])\n",
    "    axes[1].set_title(\"square_feet (after)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01096a30",
   "metadata": {},
   "source": [
    "### 8) Train/Val/Test split and standardization demo (for numerics used by KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187cbc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/15/15 split\n",
    "y = df_proc[\"price_monthly\"].astype(float)\n",
    "X = df_proc.drop(columns=[\"price_monthly\"])\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1765, random_state=42)\n",
    "\n",
    "# Fit scaler on training numeric columns\n",
    "scaler = StandardScaler()\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_val_scaled[num_cols]   = scaler.transform(X_val[num_cols])\n",
    "X_test_scaled[num_cols]  = scaler.transform(X_test[num_cols])\n",
    "\n",
    "print(\"Splits:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# Show before vs after scaling for a couple variables (if present)\n",
    "show_vars = [c for c in [\"square_feet\",\"price_per_sqft\"] if c in num_cols][:2]\n",
    "for v in show_vars:\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,3))\n",
    "    X_train[v].plot(kind=\"hist\", bins=50, ax=axes[0]); axes[0].set_title(f\"{v} (before scale)\")\n",
    "    X_train_scaled[v].plot(kind=\"hist\", bins=50, ax=axes[1]); axes[1].set_title(f\"{v} (z-scored)\")\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe12913",
   "metadata": {},
   "source": [
    "## P2–P3. Modeling (KNN, DT, RF) and hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MAPE_%\": mean_absolute_percentage_error(y_true, y_pred)*100,\n",
    "        \"RMSE\": np.sqrt(((y_true - y_pred)**2).mean()),\n",
    "        \"R2\": r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_results(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    # order columns\n",
    "    cols = [\"model\",\"params\",\"split\",\"MAE\",\"MAPE_%\",\"RMSE\",\"R2\"]\n",
    "    return df[cols] if all(c in df.columns for c in cols) else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "# Baseline (mean of y_train)\n",
    "baseline_pred = np.full_like(y_val, y_train.mean())\n",
    "m = eval_metrics(y_val, baseline_pred)\n",
    "all_results.append({\"model\":\"Baseline\",\"params\":\"mean(y_train)\",\"split\":\"Val\", **m})\n",
    "\n",
    "# --- KNN over k --- (uses scaled features)\n",
    "for k in [3,5,10]:\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    for split, X_, y_ in [(\"Train\", X_train_scaled, y_train),\n",
    "                          (\"Val\", X_val_scaled, y_val)]:\n",
    "        pred = model.predict(X_)\n",
    "        m = eval_metrics(y_, pred)\n",
    "        all_results.append({\"model\":\"KNN\",\"params\":f\"k={k}\",\"split\":split, **m})\n",
    "\n",
    "# --- Decision Tree ---\n",
    "for depth in [5,10,None]:\n",
    "    # set min_samples_leaf variations minimally to satisfy requirement\n",
    "    for minleaf in [1,5,10]:\n",
    "        model = DecisionTreeRegressor(max_depth=depth, min_samples_leaf=minleaf, random_state=42)\n",
    "        model.fit(X_train, y_train)  # trees OK on unscaled\n",
    "        for split, X_, y_ in [(\"Train\", X_train, y_train),\n",
    "                              (\"Val\", X_val, y_val)]:\n",
    "            pred = model.predict(X_)\n",
    "            m = eval_metrics(y_, pred)\n",
    "            all_results.append({\"model\":\"DecisionTree\",\"params\":f\"depth={depth},leaf={minleaf}\",\"split\":split, **m})\n",
    "\n",
    "# --- Random Forest ---\n",
    "for n in [50,100,200]:\n",
    "    for minleaf in [1,2]:\n",
    "        model = RandomForestRegressor(n_estimators=n, min_samples_leaf=minleaf, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        for split, X_, y_ in [(\"Train\", X_train, y_train),\n",
    "                              (\"Val\", X_val, y_val)]:\n",
    "            pred = model.predict(X_)\n",
    "            m = eval_metrics(y_, pred)\n",
    "            all_results.append({\"model\":\"RandomForest\",\"params\":f\"n={n},leaf={minleaf}\",\"split\":split, **m})\n",
    "\n",
    "results_df = summarize_results(all_results)\n",
    "print(\"Validation results (sample):\")\n",
    "display(results_df[results_df[\"split\"]==\"Val\"].sort_values([\"model\",\"MAE\"]).groupby(\"model\").head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a06c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {}\n",
    "for model_name in results_df[\"model\"].unique():\n",
    "    sub = results_df[(results_df[\"model\"]==model_name) & (results_df[\"split\"]==\"Val\")]\n",
    "    if len(sub)==0: \n",
    "        continue\n",
    "    row = sub.sort_values(\"MAE\").iloc[0]\n",
    "    best[model_name] = row[\"params\"]\n",
    "\n",
    "print(\"Best hyperparameters (by lowest Val MAE):\")\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c418559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain best models on Train+Val and evaluate on Test\n",
    "X_trv_scaled = pd.concat([X_train_scaled, X_val_scaled], axis=0)\n",
    "y_trv = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "X_trv = pd.concat([X_train, X_val], axis=0)\n",
    "\n",
    "final_results = []\n",
    "\n",
    "# Baseline on Test\n",
    "baseline_test = np.full_like(y_test, y_trv.mean())\n",
    "final_results.append({\"model\":\"Baseline\",\"params\":\"mean(tr+val)\",\"split\":\"Test\", **eval_metrics(y_test, baseline_test)})\n",
    "\n",
    "# KNN\n",
    "if \"KNN\" in best:\n",
    "    k = int(best[\"KNN\"].split(\"=\")[1])\n",
    "    knn = KNeighborsRegressor(n_neighbors=k).fit(X_trv_scaled, y_trv)\n",
    "    final_results.append({\"model\":\"KNN\",\"params\":best[\"KNN\"],\"split\":\"Test\",\n",
    "                          **eval_metrics(y_test, knn.predict(X_test_scaled))})\n",
    "\n",
    "# Decision Tree\n",
    "if \"DecisionTree\" in best:\n",
    "    pars = dict(x.split(\"=\") for x in best[\"DecisionTree\"].replace(\"depth=\",\"depth=\").split(\",\") )\n",
    "    depth = None if pars[\"depth\"]==\"None\" else int(pars[\"depth\"])\n",
    "    leaf = int(pars[\"leaf\"])\n",
    "    dt = DecisionTreeRegressor(max_depth=depth, min_samples_leaf=leaf, random_state=42).fit(X_trv, y_trv)\n",
    "    final_results.append({\"model\":\"DecisionTree\",\"params\":best[\"DecisionTree\"],\"split\":\"Test\",\n",
    "                          **eval_metrics(y_test, dt.predict(X_test))})\n",
    "\n",
    "# Random Forest\n",
    "if \"RandomForest\" in best:\n",
    "    pars = dict(x.split(\"=\") for x in best[\"RandomForest\"].split(\",\") )\n",
    "    n = int(pars[\"n\"])\n",
    "    leaf = int(pars[\"leaf\"])\n",
    "    rf = RandomForestRegressor(n_estimators=n, min_samples_leaf=leaf, random_state=42, n_jobs=-1).fit(X_trv, y_trv)\n",
    "    final_results.append({\"model\":\"RandomForest\",\"params\":best[\"RandomForest\"],\"split\":\"Test\",\n",
    "                          **eval_metrics(y_test, rf.predict(X_test))})\n",
    "\n",
    "final_df = pd.DataFrame(final_results)\n",
    "print(\"Final Test metrics:\")\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5978c",
   "metadata": {},
   "source": [
    "### Feature Importances (Random Forest, if selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"RandomForest\" in best:\n",
    "    pars = dict(x.split(\"=\") for x in best[\"RandomForest\"].split(\",\") )\n",
    "    n = int(pars[\"n\"]); leaf = int(pars[\"leaf\"])\n",
    "    rf = RandomForestRegressor(n_estimators=n, min_samples_leaf=leaf, random_state=42, n_jobs=-1).fit(\n",
    "        pd.concat([X_train, X_val]), pd.concat([y_train, y_val])\n",
    "    )\n",
    "    imp = pd.DataFrame({\"feature\": X_train.columns, \"importance\": rf.feature_importances_})\\\n",
    "            .sort_values(\"importance\", ascending=False)\n",
    "    display(imp.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1c996",
   "metadata": {},
   "source": [
    "---\n",
    "**References (for your convenience):**\n",
    "- scikit-learn KNN: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html  \n",
    "- scikit-learn Decision Trees: https://scikit-learn.org/stable/modules/tree.html  \n",
    "- scikit-learn RandomForestRegressor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html  \n",
    "- Original dataset: https://archive.ics.uci.edu/dataset/555/apartment+for+rent+classified\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

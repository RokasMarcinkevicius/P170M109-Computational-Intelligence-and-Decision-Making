{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CIDM Lab 1 – Simple solution (KNN, Decision Tree, Random Forest)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 1. Load data\n",
    "# --------------------\n",
    "df = pd.read_csv(\"apartments_for_rent_classified_100K.csv\")\n",
    "\n",
    "# Keep only numeric price (drop formatted strings)\n",
    "df = df.dropna(subset=[\"price\"])\n",
    "df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"price\"])\n",
    "\n",
    "# Drop unused fields\n",
    "drop_cols = [\"price_display\", \"price_type\", \"currency\", \"title\", \"body\", \"address\"]\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "# Simple derived feature: count of amenities\n",
    "if \"amenities\" in df.columns:\n",
    "    df[\"amenity_count\"] = df[\"amenities\"].fillna(\"\").apply(lambda x: len(str(x).split(\",\")))\n",
    "    df = df.drop(columns=[\"amenities\"])\n",
    "\n",
    "# Encode binary Yes/No to 1/0\n",
    "for col in [\"fee\", \"has_photo\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.lower().map({\"yes\": 1, \"no\": 0, \"thumbnail\": 1}).fillna(0)\n",
    "\n",
    "# --------------------\n",
    "# 2. Data quality report\n",
    "# --------------------\n",
    "print(\"Data Quality Report:\")\n",
    "print(df.info())\n",
    "print(df.describe(include=\"all\"))\n",
    "\n",
    "# --------------------\n",
    "# 3. Preprocessing\n",
    "# --------------------\n",
    "# Remove outliers: price > 10000 or square_feet > 8000\n",
    "if \"square_feet\" in df.columns:\n",
    "    df = df[(df[\"price\"] <= 10000) & (df[\"square_feet\"] <= 8000)]\n",
    "\n",
    "# Features/target\n",
    "y = df[\"price\"]\n",
    "X = df.drop(columns=[\"price\"])\n",
    "\n",
    "# Train/val/test split\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1765, random_state=42)\n",
    "# (≈70% train, 15% val, 15% test)\n",
    "\n",
    "# Scale continuous variables (needed for KNN)\n",
    "scaler = StandardScaler()\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_val[num_cols]   = scaler.transform(X_val[num_cols])\n",
    "X_test[num_cols]  = scaler.transform(X_test[num_cols])\n",
    "\n",
    "# --------------------\n",
    "# 4. Evaluation function\n",
    "# --------------------\n",
    "def evaluate(model, Xtr, ytr, Xv, yv, Xte, yte):\n",
    "    metrics = {}\n",
    "    for split, X_, y_ in [(\"Train\", Xtr, ytr), (\"Val\", Xv, yv), (\"Test\", Xte, yte)]:\n",
    "        pred = model.predict(X_)\n",
    "        mae = mean_absolute_error(y_, pred)\n",
    "        mape = mean_absolute_percentage_error(y_, pred) * 100\n",
    "        rmse = np.sqrt(((y_ - pred) ** 2).mean())\n",
    "        r2 = r2_score(y_, pred)\n",
    "        metrics[split] = (mae, mape, rmse, r2)\n",
    "    return metrics\n",
    "\n",
    "def print_results(name, metrics):\n",
    "    print(f\"\\n{name} results:\")\n",
    "    for split, vals in metrics.items():\n",
    "        mae, mape, rmse, r2 = vals\n",
    "        print(f\"{split}: MAE={mae:.1f}, MAPE={mape:.1f}%, RMSE={rmse:.1f}, R2={r2:.3f}\")\n",
    "\n",
    "# --------------------\n",
    "# 5. Models\n",
    "# --------------------\n",
    "\n",
    "# Baseline (mean predictor)\n",
    "y_mean = np.full_like(y_test, y_train.mean())\n",
    "print(\"\\nBaseline (mean predictor):\")\n",
    "print(f\"Test MAE={mean_absolute_error(y_test, y_mean):.1f}, \"\n",
    "      f\"MAPE={mean_absolute_percentage_error(y_test, y_mean)*100:.1f}%, \"\n",
    "      f\"RMSE={np.sqrt(((y_test - y_mean)**2).mean()):.1f}, \"\n",
    "      f\"R2={r2_score(y_test, y_mean):.3f}\")\n",
    "\n",
    "# KNN\n",
    "for k in [3, 5, 10]:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    metrics = evaluate(knn, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    print_results(f\"KNN (k={k})\", metrics)\n",
    "\n",
    "# Decision Tree\n",
    "for depth in [5, 10, None]:\n",
    "    dt = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    metrics = evaluate(dt, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    print_results(f\"Decision Tree (max_depth={depth})\", metrics)\n",
    "\n",
    "# Random Forest\n",
    "for n in [10, 50, 100]:\n",
    "    rf = RandomForestRegressor(n_estimators=n, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    metrics = evaluate(rf, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    print_results(f\"Random Forest (n_estimators={n})\", metrics)\n"
   ],
   "id": "f9e469c5409bb4c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

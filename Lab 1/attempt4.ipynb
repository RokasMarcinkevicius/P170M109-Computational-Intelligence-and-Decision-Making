{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6878c27b",
   "metadata": {},
   "source": [
    "# CIDM Lab 1 — P1→P3 (Notebook Only, Code You Can Edit)\n",
    "**Course:** P170M109 Computational Intelligence and Decision Making  \n",
    "**This notebook covers:**  \n",
    "- **P1**: Input analysis & preprocessing (before/after)  \n",
    "- **P2**: Modeling with **KNN**, **Decision Tree**, **Random Forest**  \n",
    "- **P3**: Hyperparameter selection & final results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1494730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T17:06:21.476215Z",
     "start_time": "2025-09-22T17:06:21.472236Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "plt.rcParams['figure.figsize'] = (7, 4)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "a3ac9edc",
   "metadata": {},
   "source": [
    "## P1. Data analysis and preprocessing\n",
    "### 1) Load data (robust file search)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cc748589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T17:06:21.657114Z",
     "start_time": "2025-09-22T17:06:21.497422Z"
    }
   },
   "source": [
    "def find_file_upwards(filename: str, start: Path | None = None) -> Path:\n",
    "    \"\"\"Search for `filename` in the current working directory and its parents.\n",
    "    Returns the full Path if found; raises FileNotFoundError otherwise.\"\"\"\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for base in [start, *start.parents]:\n",
    "        candidate = base / filename\n",
    "        if candidate.is_file():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find '{filename}' starting from {start}.\\n\"\n",
    "        f\"Current working directory: {Path.cwd()}\\n\"\n",
    "        \"Make sure the filename is correct and the file exists.\"\n",
    "    )\n",
    "\n",
    "# Common dataset names (100K preferred, then 10K)\n",
    "CANDIDATES = [\n",
    "    \"apartments_for_rent_classified_100K.csv\",\n",
    "    \"apartments_for_rent_classified_10K.csv\",\n",
    "]\n",
    "\n",
    "data_path = None\n",
    "for name in CANDIDATES:\n",
    "    try:\n",
    "        data_path = find_file_upwards(name)\n",
    "        print(\"Loaded file:\", data_path)\n",
    "        break\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "if data_path is None:\n",
    "    # Fallback: explicit class paths\n",
    "    for p in [Path(\"/mnt/data/apartments_for_rent_classified_100K.csv\"),\n",
    "              Path(\"/mnt/data/apartments_for_rent_classified_10K.csv\")]:\n",
    "        if p.exists():\n",
    "            data_path = p\n",
    "            print(\"Loaded file:\", data_path)\n",
    "            break\n",
    "\n",
    "if data_path is None:\n",
    "    raise FileNotFoundError(\"Dataset not found. Put the CSV near this notebook or in /mnt/data/.\")\n",
    "\n",
    "df_raw = pd.read_csv(data_path)\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "df_raw.head(3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: C:\\Users\\Rokas\\Documents\\KTU\\KTU P170M109 Computational Intelligence and Decision Making\\Lab 1\\apartments_for_rent_classified_100K.csv\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 97583: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnicodeDecodeError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 42\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m data_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     40\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mDataset not found. Put the CSV near this notebook or in /mnt/data/.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m df_raw = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mRaw shape:\u001B[39m\u001B[33m\"\u001B[39m, df_raw.shape)\n\u001B[32m     44\u001B[39m df_raw.head(\u001B[32m3\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1895\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m   1897\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1898\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmapping\u001B[49m\u001B[43m[\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1899\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1900\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001B[39m, in \u001B[36mCParserWrapper.__init__\u001B[39m\u001B[34m(self, src, **kwds)\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwds[\u001B[33m\"\u001B[39m\u001B[33mdtype_backend\u001B[39m\u001B[33m\"\u001B[39m] == \u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     91\u001B[39m     \u001B[38;5;66;03m# Fail here loudly instead of in cython after reading\u001B[39;00m\n\u001B[32m     92\u001B[39m     import_optional_dependency(\u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m \u001B[38;5;28mself\u001B[39m._reader = \u001B[43mparsers\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTextReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[38;5;28mself\u001B[39m.unnamed_cols = \u001B[38;5;28mself\u001B[39m._reader.unnamed_cols\n\u001B[32m     97\u001B[39m \u001B[38;5;66;03m# error: Cannot determine type of 'names'\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:574\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader.__cinit__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:663\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._get_header\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:874\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:891\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/parsers.pyx:2053\u001B[39m, in \u001B[36mpandas._libs.parsers.raise_parser_error\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen codecs>:325\u001B[39m, in \u001B[36mdecode\u001B[39m\u001B[34m(self, input, final)\u001B[39m\n",
      "\u001B[31mUnicodeDecodeError\u001B[39m: 'utf-8' codec can't decode byte 0x92 in position 97583: invalid start byte"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "5661a241",
   "metadata": {},
   "source": [
    "### 2) Determine feature types & Data Quality Reports (numeric vs categorical)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f35a33d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T17:07:22.969548Z",
     "start_time": "2025-09-22T17:07:22.925603Z"
    }
   },
   "source": [
    "def infer_feature_types(df: pd.DataFrame):\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cat_cols = [c for c in df.columns if c not in num_cols]\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "def dqr_numeric(df: pd.DataFrame, cols):\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        n = len(s); miss = s.isna().sum()\n",
    "        desc = s.describe()\n",
    "        rows.append({\n",
    "            \"feature\": c, \"n\": n, \"missing\": int(miss),\n",
    "            \"missing_%\": round(100*miss/n, 2),\n",
    "            \"unique\": int(s.nunique(dropna=True)),\n",
    "            \"mean\": desc.get(\"mean\", np.nan),\n",
    "            \"std\": desc.get(\"std\", np.nan),\n",
    "            \"min\": desc.get(\"min\", np.nan),\n",
    "            \"p25\": desc.get(\"25%\", np.nan),\n",
    "            \"p50\": desc.get(\"50%\", np.nan),\n",
    "            \"p75\": desc.get(\"75%\", np.nan),\n",
    "            \"max\": desc.get(\"max\", np.nan),\n",
    "            \"skew\": s.dropna().skew() if s.notna().any() else np.nan\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values([\"missing_%\",\"feature\"], ascending=[False, True])\n",
    "\n",
    "def dqr_categorical(df: pd.DataFrame, cols):\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        s = df[c].astype(\"string\")\n",
    "        n = len(s); miss = s.isna().sum()\n",
    "        vc = s.value_counts(dropna=True)\n",
    "        rows.append({\n",
    "            \"feature\": c, \"n\": n, \"missing\": int(miss),\n",
    "            \"missing_%\": round(100*miss/n, 2),\n",
    "            \"unique\": int(vc.shape[0]),\n",
    "            \"top_value\": vc.index[:1].tolist(),\n",
    "            \"top_freq\": vc.values[:1].tolist(),\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values([\"missing_%\",\"feature\"], ascending=[False, True])\n",
    "\n",
    "num_cols_raw, cat_cols_raw = infer_feature_types(df_raw)\n",
    "print(\"Numeric cols (raw):\", len(num_cols_raw))\n",
    "print(\"Categorical cols (raw):\", len(cat_cols_raw))\n",
    "\n",
    "print(\"\\nNumeric DQR (raw):\")\n",
    "display(dqr_numeric(df_raw, num_cols_raw).head(25))\n",
    "\n",
    "print(\"\\nCategorical DQR (raw):\")\n",
    "display(dqr_categorical(df_raw, cat_cols_raw).head(25))"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 42\u001B[39m\n\u001B[32m     33\u001B[39m         rows.append({\n\u001B[32m     34\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mfeature\u001B[39m\u001B[33m\"\u001B[39m: c, \u001B[33m\"\u001B[39m\u001B[33mn\u001B[39m\u001B[33m\"\u001B[39m: n, \u001B[33m\"\u001B[39m\u001B[33mmissing\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mint\u001B[39m(miss),\n\u001B[32m     35\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmissing_\u001B[39m\u001B[33m%\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mround\u001B[39m(\u001B[32m100\u001B[39m*miss/n, \u001B[32m2\u001B[39m),\n\u001B[32m   (...)\u001B[39m\u001B[32m     38\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtop_freq\u001B[39m\u001B[33m\"\u001B[39m: vc.values[:\u001B[32m1\u001B[39m].tolist(),\n\u001B[32m     39\u001B[39m         })\n\u001B[32m     40\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m pd.DataFrame(rows).sort_values([\u001B[33m\"\u001B[39m\u001B[33mmissing_\u001B[39m\u001B[33m%\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mfeature\u001B[39m\u001B[33m\"\u001B[39m], ascending=[\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m])\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m num_cols_raw, cat_cols_raw = infer_feature_types(\u001B[43mdf_raw\u001B[49m)\n\u001B[32m     43\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mNumeric cols (raw):\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(num_cols_raw))\n\u001B[32m     44\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mCategorical cols (raw):\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(cat_cols_raw))\n",
      "\u001B[31mNameError\u001B[39m: name 'df_raw' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "4ce69b13",
   "metadata": {},
   "source": [
    "### 3) Create `price_monthly` **before** cleaning for initial distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_monthly(price, ptype):\n",
    "    t = (str(ptype) if ptype is not None else \"\").strip().lower()\n",
    "    try:\n",
    "        p = float(price)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    if not np.isfinite(p) or p <= 0: return np.nan\n",
    "    if \"week\" in t:             factor = 52/12\n",
    "    elif \"fortnight\" in t or \"biweek\" in t: factor = 26/12\n",
    "    elif \"day\" in t:            factor = 30\n",
    "    elif \"year\" in t or \"annual\" in t:      factor = 1/12\n",
    "    elif \"hour\" in t:           factor = 24*30\n",
    "    else:                       factor = 1.0  # assume monthly\n",
    "    return p * factor\n",
    "\n",
    "df_before = df_raw.copy()\n",
    "if \"price\" in df_before.columns and \"price_type\" in df_before.columns:\n",
    "    df_before[\"price_monthly\"] = df_before.apply(lambda r: to_monthly(r.get(\"price\"), r.get(\"price_type\")), axis=1)\n",
    "else:\n",
    "    raise KeyError(\"Expected 'price' and 'price_type' in the dataset.\")\n",
    "\n",
    "df_before[[\"price\",\"price_type\",\"price_monthly\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06ee641",
   "metadata": {},
   "source": [
    "### 4) Distributions (BEFORE preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "df_before[\"price_monthly\"].dropna().plot(kind=\"hist\", bins=50, ax=axes[0]); axes[0].set_title(\"price_monthly (before)\")\n",
    "if \"square_feet\" in df_before.columns:\n",
    "    pd.to_numeric(df_before[\"square_feet\"], errors=\"coerce\").dropna().plot(kind=\"hist\", bins=50, ax=axes[1]); axes[1].set_title(\"square_feet (before)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Boxplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "df_before[[\"price_monthly\"]].boxplot(ax=axes[0]); axes[0].set_title(\"price_monthly (before)\")\n",
    "if \"square_feet\" in df_before.columns:\n",
    "    pd.to_numeric(df_before[\"square_feet\"], errors=\"coerce\").to_frame().boxplot(ax=axes[1]); axes[1].set_title(\"square_feet (before)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Categorical top-15 bar\n",
    "for cat in [\"state\", \"cityname\", \"region\"]:\n",
    "    if cat in df_before.columns:\n",
    "        vc = df_before[cat].astype(str).value_counts().head(15)\n",
    "        ax = vc.plot(kind=\"bar\", figsize=(10,3)); ax.set_title(f\"{cat} (top 15)\"); plt.tight_layout(); plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127d0ad",
   "metadata": {},
   "source": [
    "### 5) Derived features & ABT (Analytics Base Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6faf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_before.copy()\n",
    "\n",
    "# Binary encodings\n",
    "for col in [\"fee\", \"has_photo\"]:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = (df_clean[col].astype(str).str.lower()\n",
    "                         .map({\"yes\":1,\"no\":0,\"thumbnail\":1}).fillna(0).astype(int))\n",
    "\n",
    "# Pets flags\n",
    "if \"pets_allowed\" in df_clean.columns:\n",
    "    lower = df_clean[\"pets_allowed\"].fillna(\"\").astype(str).str.lower()\n",
    "    df_clean[\"pet_cat_allowed\"] = lower.str.contains(\"cat\").astype(int)\n",
    "    df_clean[\"pet_dog_allowed\"] = lower.str.contains(\"dog\").astype(int)\n",
    "\n",
    "# Amenity count\n",
    "if \"amenities\" in df_clean.columns:\n",
    "    df_clean[\"amenity_count\"] = df_clean[\"amenities\"].fillna(\"\").apply(lambda x: len(str(x).split(\",\"))).astype(int)\n",
    "\n",
    "# price_per_sqft\n",
    "if \"square_feet\" in df_clean.columns:\n",
    "    sf = pd.to_numeric(df_clean[\"square_feet\"], errors=\"coerce\")\n",
    "    df_clean[\"price_per_sqft\"] = df_clean[\"price_monthly\"] / sf.replace(0, np.nan)\n",
    "\n",
    "# ABT: drop text-heavy/IDs\n",
    "abt_drop = [\"id\",\"title\",\"body\",\"address\",\"amenities\",\"price_display\",\"price\",\"price_type\",\"currency\",\"source\"]\n",
    "df_abt = df_clean.drop(columns=[c for c in abt_drop if c in df_clean.columns], errors=\"ignore\")\n",
    "\n",
    "print(\"ABT candidate shape:\", df_abt.shape)\n",
    "df_abt.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91662e41",
   "metadata": {},
   "source": [
    "### 6) Preprocess (filters, missing values, outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d586559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = df_abt.copy()\n",
    "\n",
    "# currency (if available in raw)\n",
    "if \"currency\" in df_raw.columns:\n",
    "    mask_usd = df_raw[\"currency\"].astype(str).str.upper().eq(\"USD\") | df_raw[\"currency\"].isna()\n",
    "    # align by index intersection\n",
    "    df_proc = df_proc.loc[df_proc.index.intersection(mask_usd[mask_usd].index)]\n",
    "\n",
    "# coords validity\n",
    "if {\"latitude\", \"longitude\"}.issubset(df_proc.columns):\n",
    "    lat = pd.to_numeric(df_proc[\"latitude\"], errors=\"coerce\")\n",
    "    lon = pd.to_numeric(df_proc[\"longitude\"], errors=\"coerce\")\n",
    "    good = lat.between(-90, 90) & lon.between(-180, 180) & ~((lat.abs()<0.1) & (lon.abs()<0.1))\n",
    "    df_proc = df_proc[good]\n",
    "\n",
    "# square feet sanity\n",
    "if \"square_feet\" in df_proc.columns:\n",
    "    sf = pd.to_numeric(df_proc[\"square_feet\"], errors=\"coerce\")\n",
    "    df_proc = df_proc[(sf.isna()) | ((sf >= 120) & (sf <= 8000))]\n",
    "\n",
    "# outliers: top 0.5%\n",
    "q995 = df_proc[\"price_monthly\"].quantile(0.995)\n",
    "df_proc = df_proc[df_proc[\"price_monthly\"] <= q995]\n",
    "\n",
    "# fill numeric NaNs\n",
    "for c in df_proc.select_dtypes(include=[np.number]).columns:\n",
    "    df_proc[c] = df_proc[c].fillna(df_proc[c].median())\n",
    "\n",
    "# drop duplicates\n",
    "if \"id\" in df_raw.columns and \"id\" in df_proc.columns:\n",
    "    df_proc = df_proc.drop_duplicates(subset=[\"id\"])\n",
    "else:\n",
    "    df_proc = df_proc.drop_duplicates()\n",
    "\n",
    "print(\"Processed shape:\", df_proc.shape)\n",
    "df_proc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db9b21",
   "metadata": {},
   "source": [
    "### 7) Distributions (AFTER preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "df_proc[\"price_monthly\"].dropna().plot(kind=\"hist\", bins=50, ax=axes[0]); axes[0].set_title(\"price_monthly (after)\")\n",
    "if \"square_feet\" in df_proc.columns:\n",
    "    pd.to_numeric(df_proc[\"square_feet\"], errors=\"coerce\").dropna().plot(kind=\"hist\", bins=50, ax=axes[1]); axes[1].set_title(\"square_feet (after)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "df_proc[[\"price_monthly\"]].boxplot(ax=axes[0]); axes[0].set_title(\"price_monthly (after)\")\n",
    "if \"square_feet\" in df_proc.columns:\n",
    "    pd.to_numeric(df_proc[\"square_feet\"], errors=\"coerce\").to_frame().boxplot(ax=axes[1]); axes[1].set_title(\"square_feet (after)\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2538a8",
   "metadata": {},
   "source": [
    "### 8) Train/Val/Test split (70/15/15) and standardization demo (for KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021eeeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_proc[\"price_monthly\"].astype(float)\n",
    "X = df_proc.drop(columns=[\"price_monthly\"])\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1765, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_val_scaled[num_cols]   = scaler.transform(X_val[num_cols])\n",
    "X_test_scaled[num_cols]  = scaler.transform(X_test[num_cols])\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# show before vs after scaling (if available)\n",
    "cand = [c for c in [\"square_feet\",\"price_per_sqft\",\"bedrooms\",\"bathrooms\"] if c in num_cols]\n",
    "if cand:\n",
    "    v = cand[0]\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,3))\n",
    "    X_train[v].plot(kind=\"hist\", bins=50, ax=axes[0]); axes[0].set_title(f\"{v} (before scale)\")\n",
    "    pd.Series(X_train_scaled[v]).plot(kind=\"hist\", bins=50, ax=axes[1]); axes[1].set_title(f\"{v} (z-scored)\")\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c28e22",
   "metadata": {},
   "source": [
    "## P2–P3. Modeling and hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MAPE_%\": mean_absolute_percentage_error(y_true, y_pred)*100,\n",
    "        \"RMSE\": np.sqrt(((y_true - y_pred)**2).mean()),\n",
    "        \"R2\": r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Baseline (mean)\n",
    "baseline_val = np.full_like(y_val, y_train.mean())\n",
    "m = eval_metrics(y_val, baseline_val)\n",
    "all_results.append({\"model\":\"Baseline\",\"params\":\"mean(y_train)\",\"split\":\"Val\", **m})\n",
    "\n",
    "# KNN (scaled)\n",
    "for k in [3,5,10]:\n",
    "    mdl = KNeighborsRegressor(n_neighbors=k)\n",
    "    mdl.fit(X_train_scaled, y_train)\n",
    "    for split, X_, y_ in [(\"Train\", X_train_scaled, y_train), (\"Val\", X_val_scaled, y_val)]:\n",
    "        pred = mdl.predict(X_)\n",
    "        all_results.append({\"model\":\"KNN\",\"params\":f\"k={k}\",\"split\":split, **eval_metrics(y_, pred)})\n",
    "\n",
    "# Decision Tree\n",
    "for depth in [5,10,None]:\n",
    "    for leaf in [1,5,10]:\n",
    "        mdl = DecisionTreeRegressor(max_depth=depth, min_samples_leaf=leaf, random_state=42)\n",
    "        mdl.fit(X_train, y_train)\n",
    "        for split, X_, y_ in [(\"Train\", X_train, y_train), (\"Val\", X_val, y_val)]:\n",
    "            pred = mdl.predict(X_)\n",
    "            all_results.append({\"model\":\"DecisionTree\",\"params\":f\"depth={depth},leaf={leaf}\",\"split\":split, **eval_metrics(y_, pred)})\n",
    "\n",
    "# Random Forest\n",
    "for n in [50,100,200]:\n",
    "    for leaf in [1,2]:\n",
    "        mdl = RandomForestRegressor(n_estimators=n, min_samples_leaf=leaf, random_state=42, n_jobs=-1)\n",
    "        mdl.fit(X_train, y_train)\n",
    "        for split, X_, y_ in [(\"Train\", X_train, y_train), (\"Val\", X_val, y_val)]:\n",
    "            pred = mdl.predict(X_)\n",
    "            all_results.append({\"model\":\"RandomForest\",\"params\":f\"n={n},leaf={leaf}\",\"split\":split, **eval_metrics(y_, pred)})\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"Validation results (best 6 per model by MAE):\")\n",
    "display(results_df[results_df[\"split\"]==\"Val\"].sort_values([\"model\",\"MAE\"]).groupby(\"model\").head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {}\n",
    "for mdl in results_df[\"model\"].unique():\n",
    "    sub = results_df[(results_df[\"model\"]==mdl) & (results_df[\"split\"]==\"Val\")]\n",
    "    if len(sub)==0: continue\n",
    "    best[mdl] = sub.sort_values(\"MAE\").iloc[0][\"params\"]\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaaf249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain best params on Train+Val, evaluate on Test\n",
    "X_trv = pd.concat([X_train, X_val], axis=0)\n",
    "y_trv = pd.concat([y_train, y_val], axis=0)\n",
    "X_trv_scaled = pd.concat([X_train_scaled, X_val_scaled], axis=0)\n",
    "\n",
    "final = []\n",
    "\n",
    "# Baseline\n",
    "baseline_test = np.full_like(y_test, y_trv.mean())\n",
    "final.append({\"model\":\"Baseline\",\"params\":\"mean(tr+val)\",\"split\":\"Test\", **eval_metrics(y_test, baseline_test)})\n",
    "\n",
    "# KNN\n",
    "if \"KNN\" in best:\n",
    "    k = int(best[\"KNN\"].split(\"=\")[1])\n",
    "    mdl = KNeighborsRegressor(n_neighbors=k).fit(X_trv_scaled, y_trv)\n",
    "    final.append({\"model\":\"KNN\",\"params\":best[\"KNN\"],\"split\":\"Test\", **eval_metrics(y_test, mdl.predict(X_test_scaled))})\n",
    "\n",
    "# Decision Tree\n",
    "if \"DecisionTree\" in best:\n",
    "    parts = dict(p.split(\"=\") for p in best[\"DecisionTree\"].split(\",\"))\n",
    "    depth = None if parts[\"depth\"]==\"None\" else int(parts[\"depth\"])\n",
    "    leaf = int(parts[\"leaf\"])\n",
    "    mdl = DecisionTreeRegressor(max_depth=depth, min_samples_leaf=leaf, random_state=42).fit(X_trv, y_trv)\n",
    "    final.append({\"model\":\"DecisionTree\",\"params\":best[\"DecisionTree\"],\"split\":\"Test\", **eval_metrics(y_test, mdl.predict(X_test))})\n",
    "\n",
    "# Random Forest\n",
    "if \"RandomForest\" in best:\n",
    "    parts = dict(p.split(\"=\") for p in best[\"RandomForest\"].split(\",\"))\n",
    "    n = int(parts[\"n\"]); leaf = int(parts[\"leaf\"])\n",
    "    mdl = RandomForestRegressor(n_estimators=n, min_samples_leaf=leaf, random_state=42, n_jobs=-1).fit(X_trv, y_trv)\n",
    "    final.append({\"model\":\"RandomForest\",\"params\":best[\"RandomForest\"],\"split\":\"Test\", **eval_metrics(y_test, mdl.predict(X_test))})\n",
    "\n",
    "final_df = pd.DataFrame(final)\n",
    "print(\"Final Test metrics:\")\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7bfd5",
   "metadata": {},
   "source": [
    "### Random Forest feature importances (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"RandomForest\" in best:\n",
    "    parts = dict(p.split(\"=\") for p in best[\"RandomForest\"].split(\",\"))\n",
    "    n = int(parts[\"n\"]); leaf = int(parts[\"leaf\"])\n",
    "    mdl = RandomForestRegressor(n_estimators=n, min_samples_leaf=leaf, random_state=42, n_jobs=-1)\\\n",
    "            .fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))\n",
    "    imp = pd.DataFrame({\"feature\": X_train.columns, \"importance\": mdl.feature_importances_})\\\n",
    "            .sort_values(\"importance\", ascending=False)\n",
    "    display(imp.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb09a5",
   "metadata": {},
   "source": [
    "---\n",
    "### References (for convenience)\n",
    "- KNN (scikit-learn): https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html  \n",
    "- Decision Trees (scikit-learn): https://scikit-learn.org/stable/modules/tree.html  \n",
    "- Random Forest (scikit-learn): https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html  \n",
    "- Dataset: https://archive.ics.uci.edu/dataset/555/apartment+for+rent+classified\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
